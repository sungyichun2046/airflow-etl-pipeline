# ðŸ“¦ airflow-etl-pipeline
![CI](https://github.com/sungyichun2046/airflow-etl-pipeline/actions/workflows/ci.yml/badge.svg?branch=main)
![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![License](https://img.shields.io/github/license/sungyichun2046/airflow-etl-pipeline)

### ðŸ“š Overview
This project builds a scalable, production-like ETL pipeline with:

- **Apache Airflow (with web UI)** for orchestration of extract-transform-load steps, including data clean and deduplication

- **Docker** & Docker Compose for containerized setup

- **Terraform** for local infrastructure simulation

- **Pytest** for unit, integration and end2end testing

- **GitHub Actions** for CI workflows running Pytest and automated tests

### ðŸ“š Demo
![Demo](images/airflow_etl.gif)

### ðŸ“š Project Structure
```text
airflow-etl-pipeline/
â”œâ”€â”€ dags/                          # Airflow DAGs
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ etl_pipeline.py            # ETL pipeline DAG
â”‚   â””â”€â”€ etl_utils.py               # Utility functions (e.g. data cleaning)
â”œâ”€â”€ data/                          # Raw and processed data
â”‚   â”œâ”€â”€ input.parquet              # Input data
â”‚   â””â”€â”€ processed.parquet          # Output data
â”œâ”€â”€ terraform/                     # Option 2: Deploy with Terraform
â”‚   â”œâ”€â”€ provider.tf                # Docker local provider config
â”‚   â”œâ”€â”€ variables.tf               # Reusable variables
â”‚   â”œâ”€â”€ main.tf                    # Resources (containers, networks, volumes, etc.)
â”‚   â””â”€â”€ outputs.tf                 # Useful outputs after `terraform apply`
|   â”œâ”€â”€  terraform.tfvars.example  # tfvars template
â”‚   â””â”€â”€ terraform.tfvars           # tfvars 
â”œâ”€â”€ tests/                         # Unit, integration, and end-to-end tests
â”‚   â”œâ”€â”€ test_end2end.py
â”‚   â”œâ”€â”€ test_integration.py
â”‚   â””â”€â”€ test_unit.py
â”‚   â””â”€â”€ test_cleaning.py
â”œâ”€â”€ .github/                      # GitHub Actions CI workflows
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ custom-airflow.Dockerfile     # Custom Dockerfile shared by both options
â”œâ”€â”€ docker-compose.yml            # Option 1: Deploy with Docker Compose
â”‚â”€â”€ setup_airflow.sh              # bash file to automate Airflow setup steps
â””â”€â”€ README.md                    
â””â”€â”€ .env
â””â”€â”€ .env.example
â””â”€â”€ .gitignore   
â””â”€â”€ images/                       # gif on Readme
    â””â”€â”€ airflow_etl.gif
```
### ðŸ“š Run Steps

#### **Part I**: Run Airflow DAG with docker-compose.yml 
* Put `listing_raw_technical_test.parquet` in folder `/data`
* Use `.env.example` to create `.env` file, update `AIRFLOW__WEBSERVER__SECRET_KEY` value (`openssl rand -hex 32` to generate `airflow_secret_key`)
* **Set up Airflow from scratch, run the first time**: 
  * `chmod +x setup_airflow.sh`
  * `./setup_airflow.sh`
* **Run Airflow with ui**: 
  * Start the Airflow webserver and scheduler: `docker-compose up -d`
  * Access the Airflow UI at: `http://localhost:8080` and run ETL (username: admin and password: admin created during previous step)
 

#### **Part II** Run Tests (Unit, Integration, End-to-End):
    docker-compose run --rm airflow-webserver bash -c "PYTHONPATH=/opt/airflow pytest tests/"

#### **Part III** Local production-like Airflow DAG with Terraform 
* To clean up all resources created by docker-compose: `docker-compose down -v`
* Install Terraform via one of two methods:

  * Method 1: Follow the official HashiCorp tutorial: https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli

  * Method 2:  Manual download and install:

  ```
  1) wget https://releases.hashicorp.com/terraform/1.5.7/terraform_1.5.7_linux_amd64.zip
  2) unzip terraform_1.5.7_linux_amd64.zip
  3) sudo mv terraform /usr/local/bin/
  4) terraform -version
  ```
* Create `terraform.tfvars` based `terraform.tfvars.example` in folder `terraform` (`openssl rand -hex 32` to generate `airflow_secret_key`)
* Run Airflow with terraform
  ```
  1) cd terraform
  2) terraform init
  3) terraform apply -var-file="terraform.tfvars"
  ```
* Access the Airflow UI at: `http://localhost:8080` and run ETL (username: admin and password: admin, variables in main.tf)
* To clean up all resources created by Terraform: `terraform destroy -var-file="terraform.tfvars" -auto-approve` 

### ðŸ“š Useful commands
* Go inside the container: `docker exec -it <CONTAINER NAME> bash` (e.g. `docker exec -it airflow-etl-pipeline-airflow-webserver-1 bash`)
* See logs: `docker logs <CONTAINER NAME>`

### ðŸ“š Recommended Debug Workflow
* Go inside the container: `docker exec -it <CONTAINER NAME> bash` 
* Check if the scheduler is running: `airflow scheduler health`
```
Expected output: 
{"metadatabase": {"status": "healthy"}, "scheduler": {"status": "healthy"}}
```
* (If the scheduler is not running) Start the scheduler: `airflow scheduler`
* List available DAGs: `airflow dags list` 
```commandline
dag_id          | filepath        | owner   | paused
================+=================+=========+=======
etl_parquet_dag | etl_pipeline.py | airflow | False 
```
* Check if the DAG is paused : `airflow dags list | grep <dag_id>`
** If paused: `airflow dags unpause <dag_id>`
* Trigger the DAG manually: `airflow dags trigger <dag_id>`
* Monitor active DAG runs: `airflow dags list-runs -d <dag_id>`
* Check task tree and statuses: `airflow tasks list <dag_id> --tree`
* Follow task logs (live or post-execution): `airflow tasks logs <dag_id> <task_id> <execution_date>`

### ðŸ“š Next steps
* Deduplication based on cosine similarity or using a vector database for intelligent similarity searches on addresses or descriptions.
* Better manage secrets using a specialized tool.
* Implement monitoring and alerting.
* Use Kubernetes for scaling in production.
* Optimize transformation steps with parallel processing (multithreading, multiprocessing, Spark, or Dask) to speed up ETL on large datasets.

### ðŸ“š Other tools to consider:
* Upgrade from LocalExecutor to CeleryExecutor for parallel task execution, then to KubernetesExecutor for scalable distributed execution.
* Secret managers (Vault, AWS Secrets Manager) for security
* DVC for data versioning
* Kafka for real-time ingestion
* ELK/Grafana for monitoring and alerting
* pytest-airflow for better DAG testing support or moto to mock AWS services.
* Handle big data with Spark and Airflow for distributed processing and better scalability


